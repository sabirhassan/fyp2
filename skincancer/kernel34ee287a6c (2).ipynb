{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from numpy.random import seed\nseed(101)\n#from tensorflow import set_random_seed\n#set_random_seed(101)\n\nimport pandas as pd\nimport numpy as np\n#import keras\n#from keras import backend as K\n\nimport tensorflow\ntensorflow.random.set_seed(101)\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n\nimport os\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport itertools\nimport shutil\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir('../input')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a new directory\nbase_dir = 'base_dir'\nos.mkdir(base_dir)\n\n\n#[CREATE FOLDERS INSIDE THE BASE DIRECTORY]\n\n# now we create 7 folders inside 'base_dir':\n\n# train_dir\n    # nv\n    # mel\n    # bkl\n    # bcc\n    # akiec\n    # vasc\n    # df\n \n# val_dir\n    # nv\n    # mel\n    # bkl\n    # bcc\n    # akiec\n    # vasc\n    # df\n\n# create a path to 'base_dir' to which we will join the names of the new folders\n# train_dir\ntrain_dir = os.path.join(base_dir, 'train_dir')\nos.mkdir(train_dir)\n\n# val_dir\nval_dir = os.path.join(base_dir, 'val_dir')\nos.mkdir(val_dir)\n\n\n# [CREATE FOLDERS INSIDE THE TRAIN, VALIDATION AND TEST FOLDERS]\n# Inside each folder we create seperate folders for each class\n\n# create new folders inside train_dir\nnv = os.path.join(train_dir, 'nv')\nos.mkdir(nv)\nmel = os.path.join(train_dir, 'mel')\nos.mkdir(mel)\nbkl = os.path.join(train_dir, 'bkl')\nos.mkdir(bkl)\nbcc = os.path.join(train_dir, 'bcc')\nos.mkdir(bcc)\nakiec = os.path.join(train_dir, 'akiec')\nos.mkdir(akiec)\nvasc = os.path.join(train_dir, 'vasc')\nos.mkdir(vasc)\ndf = os.path.join(train_dir, 'df')\nos.mkdir(df)\n\n\n\n# create new folders inside val_dir\nnv = os.path.join(val_dir, 'nv')\nos.mkdir(nv)\nmel = os.path.join(val_dir, 'mel')\nos.mkdir(mel)\nbkl = os.path.join(val_dir, 'bkl')\nos.mkdir(bkl)\nbcc = os.path.join(val_dir, 'bcc')\nos.mkdir(bcc)\nakiec = os.path.join(val_dir, 'akiec')\nos.mkdir(akiec)\nvasc = os.path.join(val_dir, 'vasc')\nos.mkdir(vasc)\ndf = os.path.join(val_dir, 'df')\nos.mkdir(df)","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_data = pd.read_csv('../input/skin-cancer-mnist-ham10000/HAM10000_metadata.csv')\n\ndf_data.head()\n","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"     lesion_id      image_id   dx dx_type   age   sex localization\n0  HAM_0000118  ISIC_0027419  bkl   histo  80.0  male        scalp\n1  HAM_0000118  ISIC_0025030  bkl   histo  80.0  male        scalp\n2  HAM_0002730  ISIC_0026769  bkl   histo  80.0  male        scalp\n3  HAM_0002730  ISIC_0025661  bkl   histo  80.0  male        scalp\n4  HAM_0001466  ISIC_0031633  bkl   histo  75.0  male          ear","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lesion_id</th>\n      <th>image_id</th>\n      <th>dx</th>\n      <th>dx_type</th>\n      <th>age</th>\n      <th>sex</th>\n      <th>localization</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>HAM_0000118</td>\n      <td>ISIC_0027419</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>80.0</td>\n      <td>male</td>\n      <td>scalp</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>HAM_0000118</td>\n      <td>ISIC_0025030</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>80.0</td>\n      <td>male</td>\n      <td>scalp</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>HAM_0002730</td>\n      <td>ISIC_0026769</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>80.0</td>\n      <td>male</td>\n      <td>scalp</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>HAM_0002730</td>\n      <td>ISIC_0025661</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>80.0</td>\n      <td>male</td>\n      <td>scalp</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>HAM_0001466</td>\n      <td>ISIC_0031633</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>75.0</td>\n      <td>male</td>\n      <td>ear</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this will tell us how many images are associated with each lesion_id\ndf = df_data.groupby('lesion_id').count()\n\n# now we filter out lesion_id's that have only one image associated with it\ndf = df[df['image_id'] == 1]\n\ndf.reset_index(inplace=True)\n\ndf.head()","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"     lesion_id  image_id  dx  dx_type  age  sex  localization\n0  HAM_0000001         1   1        1    1    1             1\n1  HAM_0000003         1   1        1    1    1             1\n2  HAM_0000004         1   1        1    1    1             1\n3  HAM_0000007         1   1        1    1    1             1\n4  HAM_0000008         1   1        1    1    1             1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lesion_id</th>\n      <th>image_id</th>\n      <th>dx</th>\n      <th>dx_type</th>\n      <th>age</th>\n      <th>sex</th>\n      <th>localization</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>HAM_0000001</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>HAM_0000003</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>HAM_0000004</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>HAM_0000007</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>HAM_0000008</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# here we identify lesion_id's that have duplicate images and those that have only\n# one image.\n\ndef identify_duplicates(x):\n    \n    unique_list = list(df['lesion_id'])\n    \n    if x in unique_list:\n        return 'no_duplicates'\n    else:\n        return 'has_duplicates'\n    \n# create a new colum that is a copy of the lesion_id column\ndf_data['duplicates'] = df_data['lesion_id']\n# apply the function to this new column\ndf_data['duplicates'] = df_data['duplicates'].apply(identify_duplicates)\n\ndf_data.head()\n","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"     lesion_id      image_id   dx dx_type   age   sex localization  \\\n0  HAM_0000118  ISIC_0027419  bkl   histo  80.0  male        scalp   \n1  HAM_0000118  ISIC_0025030  bkl   histo  80.0  male        scalp   \n2  HAM_0002730  ISIC_0026769  bkl   histo  80.0  male        scalp   \n3  HAM_0002730  ISIC_0025661  bkl   histo  80.0  male        scalp   \n4  HAM_0001466  ISIC_0031633  bkl   histo  75.0  male          ear   \n\n       duplicates  \n0  has_duplicates  \n1  has_duplicates  \n2  has_duplicates  \n3  has_duplicates  \n4  has_duplicates  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lesion_id</th>\n      <th>image_id</th>\n      <th>dx</th>\n      <th>dx_type</th>\n      <th>age</th>\n      <th>sex</th>\n      <th>localization</th>\n      <th>duplicates</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>HAM_0000118</td>\n      <td>ISIC_0027419</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>80.0</td>\n      <td>male</td>\n      <td>scalp</td>\n      <td>has_duplicates</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>HAM_0000118</td>\n      <td>ISIC_0025030</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>80.0</td>\n      <td>male</td>\n      <td>scalp</td>\n      <td>has_duplicates</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>HAM_0002730</td>\n      <td>ISIC_0026769</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>80.0</td>\n      <td>male</td>\n      <td>scalp</td>\n      <td>has_duplicates</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>HAM_0002730</td>\n      <td>ISIC_0025661</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>80.0</td>\n      <td>male</td>\n      <td>scalp</td>\n      <td>has_duplicates</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>HAM_0001466</td>\n      <td>ISIC_0031633</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>75.0</td>\n      <td>male</td>\n      <td>ear</td>\n      <td>has_duplicates</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df_data[df_data['duplicates'] == 'no_duplicates']\ny = df['dx']\n\n_, df_val = train_test_split(df, test_size=0.17, random_state=101, stratify=y)\n","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This set will be df_data excluding all rows that are in the val set\n\n# This function identifies if an image is part of the train\n# or val set.\ndef identify_val_rows(x):\n    # create a list of all the lesion_id's in the val set\n    val_list = list(df_val['image_id'])\n    \n    if str(x) in val_list:\n        return 'val'\n    else:\n        return 'train'\n\n# identify train and val rows\n\n# create a new colum that is a copy of the image_id column\ndf_data['train_or_val'] = df_data['image_id']\n# apply the function to this new column\ndf_data['train_or_val'] = df_data['train_or_val'].apply(identify_val_rows)\n   \n# filter out train rows\ndf_train = df_data[df_data['train_or_val'] == 'train']\n\n\nprint(len(df_train))\nprint(len(df_val))","execution_count":7,"outputs":[{"output_type":"stream","text":"9077\n938\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set the image_id as the index in df_data\ndf_data.set_index('image_id', inplace=True)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get a list of images in each of the two folders\nfolder_1 = os.listdir('../input/skin-cancer-mnist-ham10000/ham10000_images_part_1')\nfolder_2 = os.listdir('../input/skin-cancer-mnist-ham10000/ham10000_images_part_2')\n\n# Get a list of train and val images\ntrain_list = list(df_train['image_id'])\nval_list = list(df_val['image_id'])\n\n\n\n# Transfer the train images\n\nfor image in train_list:\n    \n    fname = image + '.jpg'\n    label = df_data.loc[image,'dx']\n    \n    if fname in folder_1:\n        # source path to image\n        src = os.path.join('../input/skin-cancer-mnist-ham10000/ham10000_images_part_1', fname)\n        # destination path to image\n        dst = os.path.join(train_dir, label, fname)\n        # copy the image from the source to the destination\n        shutil.copyfile(src, dst)\n\n    if fname in folder_2:\n        # source path to image\n        src = os.path.join('../input/skin-cancer-mnist-ham10000/ham10000_images_part_2', fname)\n        # destination path to image\n        dst = os.path.join(train_dir, label, fname)\n        # copy the image from the source to the destination\n        shutil.copyfile(src, dst)\n\n\n# Transfer the val images\n\nfor image in val_list:\n    \n    fname = image + '.jpg'\n    label = df_data.loc[image,'dx']\n    \n    if fname in folder_1:\n        # source path to image\n        src = os.path.join('../input/skin-cancer-mnist-ham10000/ham10000_images_part_1', fname)\n        # destination path to image\n        dst = os.path.join(val_dir, label, fname)\n        # copy the image from the source to the destination\n        shutil.copyfile(src, dst)\n\n    if fname in folder_2:\n        # source path to image\n        src = os.path.join('../input/skin-cancer-mnist-ham10000/ham10000_images_part_2', fname)\n        # destination path to image\n        dst = os.path.join(val_dir, label, fname)\n        # copy the image from the source to the destination\n        shutil.copyfile(src, dst)","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check how many train images we have in each folder\n\nprint(len(os.listdir('base_dir/train_dir/nv')))\nprint(len(os.listdir('base_dir/train_dir/mel')))\nprint(len(os.listdir('base_dir/train_dir/bkl')))\nprint(len(os.listdir('base_dir/train_dir/bcc')))\nprint(len(os.listdir('base_dir/train_dir/akiec')))\nprint(len(os.listdir('base_dir/train_dir/vasc')))\nprint(len(os.listdir('base_dir/train_dir/df')))","execution_count":10,"outputs":[{"output_type":"stream","text":"5954\n1074\n1024\n484\n301\n131\n109\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check how many val images we have in each folder\n\nprint(len(os.listdir('base_dir/val_dir/nv')))\nprint(len(os.listdir('base_dir/val_dir/mel')))\nprint(len(os.listdir('base_dir/val_dir/bkl')))\nprint(len(os.listdir('base_dir/val_dir/bcc')))\nprint(len(os.listdir('base_dir/val_dir/akiec')))\nprint(len(os.listdir('base_dir/val_dir/vasc')))\nprint(len(os.listdir('base_dir/val_dir/df')))","execution_count":11,"outputs":[{"output_type":"stream","text":"751\n39\n75\n30\n26\n11\n6\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# note that we are not augmenting class 'nv'\nclass_list = ['mel','bkl','bcc','akiec','vasc','df']\n\nfor item in class_list:\n    \n    # We are creating temporary directories here because we delete these directories later\n    # create a base dir\n    aug_dir = 'aug_dir'\n    os.mkdir(aug_dir)\n    # create a dir within the base dir to store images of the same class\n    img_dir = os.path.join(aug_dir, 'img_dir')\n    os.mkdir(img_dir)\n\n    # Choose a class\n    img_class = item\n\n    # list all images in that directory\n    img_list = os.listdir('base_dir/train_dir/' + img_class)\n\n    # Copy images from the class train dir to the img_dir e.g. class 'mel'\n    for fname in img_list:\n            # source path to image\n            src = os.path.join('base_dir/train_dir/' + img_class, fname)\n            # destination path to image\n            dst = os.path.join(img_dir, fname)\n            # copy the image from the source to the destination\n            shutil.copyfile(src, dst)\n\n\n    # point to a dir containing the images and not to the images themselves\n    path = aug_dir\n    save_path = 'base_dir/train_dir/' + img_class\n\n    # Create a data generator\n    datagen = ImageDataGenerator(\n        rotation_range=180,\n        width_shift_range=0.1,\n        height_shift_range=0.1,\n        zoom_range=0.1,\n        horizontal_flip=True,\n        vertical_flip=True,\n        #brightness_range=(0.9,1.1),\n        fill_mode='nearest')\n\n    batch_size = 50\n\n    aug_datagen = datagen.flow_from_directory(path,\n                                           save_to_dir=save_path,\n                                           save_format='jpg',\n                                                    target_size=(224,224),\n                                                    batch_size=batch_size)\n\n\n\n    # Generate the augmented images and add them to the training folders\n    \n    ###########\n    \n    num_aug_images_wanted = 6000 # total number of images we want to have in each class\n    \n    ###########\n    \n    num_files = len(os.listdir(img_dir))\n    num_batches = int(np.ceil((num_aug_images_wanted-num_files)/batch_size))\n\n    # run the generator and create about 6000 augmented images\n    for i in range(0,num_batches):\n\n        imgs, labels = next(aug_datagen)\n        \n    # delete temporary directory with the raw image files\n    shutil.rmtree('aug_dir')","execution_count":12,"outputs":[{"output_type":"stream","text":"Found 1074 images belonging to 1 classes.\nFound 1024 images belonging to 1 classes.\nFound 484 images belonging to 1 classes.\nFound 301 images belonging to 1 classes.\nFound 131 images belonging to 1 classes.\nFound 109 images belonging to 1 classes.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check how many train images we now have in each folder.\n# This is the original images plus the augmented images.\n\nprint(len(os.listdir('base_dir/train_dir/nv')))\nprint(len(os.listdir('base_dir/train_dir/mel')))\nprint(len(os.listdir('base_dir/train_dir/bkl')))\nprint(len(os.listdir('base_dir/train_dir/bcc')))\nprint(len(os.listdir('base_dir/train_dir/akiec')))\nprint(len(os.listdir('base_dir/train_dir/vasc')))\nprint(len(os.listdir('base_dir/train_dir/df')))","execution_count":13,"outputs":[{"output_type":"stream","text":"5954\n5920\n5920\n5858\n5217\n5290\n4410\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check how many val images we have in each folder.\n\nprint(len(os.listdir('base_dir/val_dir/nv')))\nprint(len(os.listdir('base_dir/val_dir/mel')))\nprint(len(os.listdir('base_dir/val_dir/bkl')))\nprint(len(os.listdir('base_dir/val_dir/bcc')))\nprint(len(os.listdir('base_dir/val_dir/akiec')))\nprint(len(os.listdir('base_dir/val_dir/vasc')))\nprint(len(os.listdir('base_dir/val_dir/df')))","execution_count":14,"outputs":[{"output_type":"stream","text":"751\n39\n75\n30\n26\n11\n6\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_path = 'base_dir/train_dir'\nvalid_path = 'base_dir/val_dir'\n\nnum_train_samples = len(df_train)\nnum_val_samples = len(df_val)\ntrain_batch_size = 10\nval_batch_size = 10\nimage_size = 224\n\ntrain_steps = np.ceil(num_train_samples / train_batch_size)\nval_steps = np.ceil(num_val_samples / val_batch_size)","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datagen = ImageDataGenerator(\n    preprocessing_function= \\\n    tensorflow.keras.applications.mobilenet.preprocess_input)\n\ntrain_batches = datagen.flow_from_directory(train_path,\n                                            target_size=(image_size,image_size),\n                                            batch_size=train_batch_size)\n\nvalid_batches = datagen.flow_from_directory(valid_path,\n                                            target_size=(image_size,image_size),\n                                            batch_size=val_batch_size)\n\n# Note: shuffle=False causes the test dataset to not be shuffled\ntest_batches = datagen.flow_from_directory(valid_path,\n                                            target_size=(image_size,image_size),\n                                            batch_size=1,\n                                            shuffle=False)","execution_count":16,"outputs":[{"output_type":"stream","text":"Found 38569 images belonging to 7 classes.\nFound 938 images belonging to 7 classes.\nFound 938 images belonging to 7 classes.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"mobile = tensorflow.keras.applications.mobilenet.MobileNet()","execution_count":17,"outputs":[{"output_type":"stream","text":"Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.6/mobilenet_1_0_224_tf.h5\n17227776/17225924 [==============================] - 1s 0us/step\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CREATE THE MODEL ARCHITECTURE\n\n# Exclude the last 5 layers of the above model.\n# This will include all layers up to and including global_average_pooling2d_1\nx = mobile.layers[-6].output\n\n# Create a new dense layer for predictions\n# 7 corresponds to the number of classes\nx = Dropout(0.25)(x)\npredictions = Dense(7, activation='softmax')(x)\n\n# inputs=mobile.input selects the input layer, outputs=predictions refers to the\n# dense layer we created above.\n\nmodel = Model(inputs=mobile.input, outputs=predictions)","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We need to choose how many layers we actually want to be trained.\n\n# Here we are freezing the weights of all layers except the\n# last 23 layers in the new model.\n# The last 23 layers of the model will be trained.\n\nfor layer in model.layers[:-23]:\n    layer.trainable = False","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define Top2 and Top3 Accuracy\n\nfrom tensorflow.keras.metrics import categorical_accuracy, top_k_categorical_accuracy\n\ndef top_3_accuracy(y_true, y_pred):\n    return top_k_categorical_accuracy(y_true, y_pred, k=3)\n\ndef top_2_accuracy(y_true, y_pred):\n    return top_k_categorical_accuracy(y_true, y_pred, k=2)","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(Adam(lr=0.01), loss='categorical_crossentropy', \n              metrics=[categorical_accuracy, top_2_accuracy, top_3_accuracy])","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add weights to try to make the model more sensitive to melanoma\n\nclass_weights={\n    0: 1.0, # akiec\n    1: 1.0, # bcc\n    2: 1.0, # bkl\n    3: 1.0, # df\n    4: 3.0, # mel # Try to make the model more sensitive to Melanoma.\n    5: 1.0, # nv\n    6: 1.0, # vasc\n}","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filepath = \"model.h5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_top_3_accuracy', verbose=1, \n                             save_best_only=True, mode='max')\n\nreduce_lr = ReduceLROnPlateau(monitor='val_top_3_accuracy', factor=0.5, patience=2, \n                                   verbose=1, mode='max', min_lr=0.00001)\n                              \n                              \ncallbacks_list = [checkpoint, reduce_lr]\n\nhistory = model.fit_generator(train_batches, steps_per_epoch=train_steps, \n                              class_weight=class_weights,\n                    validation_data=valid_batches,\n                    validation_steps=val_steps,\n                    epochs=30, verbose=1,\n                   callbacks=callbacks_list)","execution_count":23,"outputs":[{"output_type":"stream","text":"Train for 908.0 steps, validate for 94.0 steps\nEpoch 1/30\n906/908 [============================>.] - ETA: 0s - loss: 1.7646 - categorical_accuracy: 0.5062 - top_2_accuracy: 0.7109 - top_3_accuracy: 0.8371\nEpoch 00001: val_top_3_accuracy improved from -inf to 0.93497, saving model to model.h5\n908/908 [==============================] - 59s 65ms/step - loss: 1.7636 - categorical_accuracy: 0.5062 - top_2_accuracy: 0.7110 - top_3_accuracy: 0.8373 - val_loss: 1.3433 - val_categorical_accuracy: 0.8134 - val_top_2_accuracy: 0.8795 - val_top_3_accuracy: 0.9350\nEpoch 2/30\n907/908 [============================>.] - ETA: 0s - loss: 1.2724 - categorical_accuracy: 0.6182 - top_2_accuracy: 0.8108 - top_3_accuracy: 0.9116\nEpoch 00002: val_top_3_accuracy did not improve from 0.93497\n908/908 [==============================] - 53s 59ms/step - loss: 1.2717 - categorical_accuracy: 0.6184 - top_2_accuracy: 0.8109 - top_3_accuracy: 0.9117 - val_loss: 2.9289 - val_categorical_accuracy: 0.2377 - val_top_2_accuracy: 0.6087 - val_top_3_accuracy: 0.8124\nEpoch 3/30\n907/908 [============================>.] - ETA: 0s - loss: 1.1655 - categorical_accuracy: 0.6428 - top_2_accuracy: 0.8364 - top_3_accuracy: 0.9322 ETA: 1s - loss: 1.1674 - categorical_accuracy: 0.6414 - top_2\nEpoch 00003: val_top_3_accuracy did not improve from 0.93497\n\nEpoch 00003: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n908/908 [==============================] - 52s 57ms/step - loss: 1.1652 - categorical_accuracy: 0.6428 - top_2_accuracy: 0.8364 - top_3_accuracy: 0.9323 - val_loss: 1.3041 - val_categorical_accuracy: 0.8102 - val_top_2_accuracy: 0.8763 - val_top_3_accuracy: 0.9062\nEpoch 4/30\n906/908 [============================>.] - ETA: 0s - loss: 0.8896 - categorical_accuracy: 0.7203 - top_2_accuracy: 0.8896 - top_3_accuracy: 0.9597\nEpoch 00004: val_top_3_accuracy did not improve from 0.93497\n908/908 [==============================] - 51s 56ms/step - loss: 0.8891 - categorical_accuracy: 0.7203 - top_2_accuracy: 0.8896 - top_3_accuracy: 0.9598 - val_loss: 0.9508 - val_categorical_accuracy: 0.8220 - val_top_2_accuracy: 0.8859 - val_top_3_accuracy: 0.9296\nEpoch 5/30\n906/908 [============================>.] - ETA: 0s - loss: 0.8277 - categorical_accuracy: 0.7434 - top_2_accuracy: 0.9099 - top_3_accuracy: 0.9660\nEpoch 00005: val_top_3_accuracy did not improve from 0.93497\n\nEpoch 00005: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n908/908 [==============================] - 50s 56ms/step - loss: 0.8269 - categorical_accuracy: 0.7438 - top_2_accuracy: 0.9101 - top_3_accuracy: 0.9661 - val_loss: 1.5474 - val_categorical_accuracy: 0.5075 - val_top_2_accuracy: 0.8401 - val_top_3_accuracy: 0.9254\nEpoch 6/30\n907/908 [============================>.] - ETA: 0s - loss: 0.6640 - categorical_accuracy: 0.8006 - top_2_accuracy: 0.9358 - top_3_accuracy: 0.9806\nEpoch 00006: val_top_3_accuracy improved from 0.93497 to 0.94243, saving model to model.h5\n908/908 [==============================] - 50s 56ms/step - loss: 0.6650 - categorical_accuracy: 0.8003 - top_2_accuracy: 0.9356 - top_3_accuracy: 0.9805 - val_loss: 0.7773 - val_categorical_accuracy: 0.8273 - val_top_2_accuracy: 0.8998 - val_top_3_accuracy: 0.9424\nEpoch 7/30\n906/908 [============================>.] - ETA: 0s - loss: 0.6053 - categorical_accuracy: 0.8164 - top_2_accuracy: 0.9411 - top_3_accuracy: 0.9841\nEpoch 00007: val_top_3_accuracy did not improve from 0.94243\n908/908 [==============================] - 51s 56ms/step - loss: 0.6044 - categorical_accuracy: 0.8167 - top_2_accuracy: 0.9412 - top_3_accuracy: 0.9841 - val_loss: 0.8131 - val_categorical_accuracy: 0.7783 - val_top_2_accuracy: 0.8838 - val_top_3_accuracy: 0.9424\nEpoch 8/30\n906/908 [============================>.] - ETA: 0s - loss: 0.5698 - categorical_accuracy: 0.8345 - top_2_accuracy: 0.9487 - top_3_accuracy: 0.9862\nEpoch 00008: val_top_3_accuracy did not improve from 0.94243\n\nEpoch 00008: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n908/908 [==============================] - 52s 57ms/step - loss: 0.5707 - categorical_accuracy: 0.8345 - top_2_accuracy: 0.9485 - top_3_accuracy: 0.9860 - val_loss: 0.9830 - val_categorical_accuracy: 0.7260 - val_top_2_accuracy: 0.8774 - val_top_3_accuracy: 0.9382\nEpoch 9/30\n907/908 [============================>.] - ETA: 0s - loss: 0.4720 - categorical_accuracy: 0.8549 - top_2_accuracy: 0.9589 - top_3_accuracy: 0.9904\nEpoch 00009: val_top_3_accuracy did not improve from 0.94243\n908/908 [==============================] - 50s 55ms/step - loss: 0.4716 - categorical_accuracy: 0.8551 - top_2_accuracy: 0.9589 - top_3_accuracy: 0.9904 - val_loss: 0.9170 - val_categorical_accuracy: 0.7932 - val_top_2_accuracy: 0.8881 - val_top_3_accuracy: 0.9339\nEpoch 10/30\n906/908 [============================>.] - ETA: 0s - loss: 0.4068 - categorical_accuracy: 0.8805 - top_2_accuracy: 0.9679 - top_3_accuracy: 0.9919\nEpoch 00010: val_top_3_accuracy did not improve from 0.94243\n\nEpoch 00010: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n908/908 [==============================] - 50s 56ms/step - loss: 0.4062 - categorical_accuracy: 0.8806 - top_2_accuracy: 0.9680 - top_3_accuracy: 0.9920 - val_loss: 0.9854 - val_categorical_accuracy: 0.7569 - val_top_2_accuracy: 0.8827 - val_top_3_accuracy: 0.9414\nEpoch 11/30\n907/908 [============================>.] - ETA: 0s - loss: 0.3646 - categorical_accuracy: 0.8937 - top_2_accuracy: 0.9735 - top_3_accuracy: 0.9945\nEpoch 00011: val_top_3_accuracy did not improve from 0.94243\n908/908 [==============================] - 51s 56ms/step - loss: 0.3643 - categorical_accuracy: 0.8938 - top_2_accuracy: 0.9736 - top_3_accuracy: 0.9945 - val_loss: 1.1439 - val_categorical_accuracy: 0.7281 - val_top_2_accuracy: 0.8785 - val_top_3_accuracy: 0.9382\nEpoch 12/30\n906/908 [============================>.] - ETA: 0s - loss: 0.3248 - categorical_accuracy: 0.9051 - top_2_accuracy: 0.9767 - top_3_accuracy: 0.9949\nEpoch 00012: val_top_3_accuracy did not improve from 0.94243\n\nEpoch 00012: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n908/908 [==============================] - 51s 56ms/step - loss: 0.3249 - categorical_accuracy: 0.9050 - top_2_accuracy: 0.9768 - top_3_accuracy: 0.9949 - val_loss: 1.0462 - val_categorical_accuracy: 0.7505 - val_top_2_accuracy: 0.8827 - val_top_3_accuracy: 0.9403\nEpoch 13/30\n906/908 [============================>.] - ETA: 0s - loss: 0.2799 - categorical_accuracy: 0.9213 - top_2_accuracy: 0.9822 - top_3_accuracy: 0.9969\nEpoch 00013: val_top_3_accuracy did not improve from 0.94243\n908/908 [==============================] - 50s 55ms/step - loss: 0.2795 - categorical_accuracy: 0.9214 - top_2_accuracy: 0.9823 - top_3_accuracy: 0.9969 - val_loss: 1.1641 - val_categorical_accuracy: 0.7217 - val_top_2_accuracy: 0.8742 - val_top_3_accuracy: 0.9339\nEpoch 14/30\n907/908 [============================>.] - ETA: 0s - loss: 0.2716 - categorical_accuracy: 0.9213 - top_2_accuracy: 0.9816 - top_3_accuracy: 0.9963\nEpoch 00014: val_top_3_accuracy improved from 0.94243 to 0.94456, saving model to model.h5\n908/908 [==============================] - 51s 56ms/step - loss: 0.2714 - categorical_accuracy: 0.9214 - top_2_accuracy: 0.9816 - top_3_accuracy: 0.9963 - val_loss: 1.0706 - val_categorical_accuracy: 0.7548 - val_top_2_accuracy: 0.8881 - val_top_3_accuracy: 0.9446\nEpoch 15/30\n907/908 [============================>.] - ETA: 0s - loss: 0.2419 - categorical_accuracy: 0.9332 - top_2_accuracy: 0.9852 - top_3_accuracy: 0.9978 ETA: 1s - loss: 0.2446 - categorical_accuracy: 0.9317 - top_2_acc\nEpoch 00015: val_top_3_accuracy did not improve from 0.94456\n908/908 [==============================] - 50s 55ms/step - loss: 0.2421 - categorical_accuracy: 0.9329 - top_2_accuracy: 0.9852 - top_3_accuracy: 0.9978 - val_loss: 1.2244 - val_categorical_accuracy: 0.7036 - val_top_2_accuracy: 0.8678 - val_top_3_accuracy: 0.9296\nEpoch 16/30\n","name":"stdout"},{"output_type":"stream","text":"907/908 [============================>.] - ETA: 0s - loss: 0.2184 - categorical_accuracy: 0.9399 - top_2_accuracy: 0.9875 - top_3_accuracy: 0.9982\nEpoch 00016: val_top_3_accuracy did not improve from 0.94456\n\nEpoch 00016: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n908/908 [==============================] - 50s 55ms/step - loss: 0.2184 - categorical_accuracy: 0.9400 - top_2_accuracy: 0.9876 - top_3_accuracy: 0.9982 - val_loss: 1.0843 - val_categorical_accuracy: 0.7708 - val_top_2_accuracy: 0.8870 - val_top_3_accuracy: 0.9424\nEpoch 17/30\n906/908 [============================>.] - ETA: 0s - loss: 0.2060 - categorical_accuracy: 0.9438 - top_2_accuracy: 0.9889 - top_3_accuracy: 0.9976\nEpoch 00017: val_top_3_accuracy did not improve from 0.94456\n908/908 [==============================] - 49s 54ms/step - loss: 0.2058 - categorical_accuracy: 0.9437 - top_2_accuracy: 0.9889 - top_3_accuracy: 0.9976 - val_loss: 1.0640 - val_categorical_accuracy: 0.7761 - val_top_2_accuracy: 0.8817 - val_top_3_accuracy: 0.9360\nEpoch 18/30\n906/908 [============================>.] - ETA: 0s - loss: 0.2034 - categorical_accuracy: 0.9434 - top_2_accuracy: 0.9891 - top_3_accuracy: 0.9977\nEpoch 00018: val_top_3_accuracy did not improve from 0.94456\n\nEpoch 00018: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n908/908 [==============================] - 49s 54ms/step - loss: 0.2031 - categorical_accuracy: 0.9434 - top_2_accuracy: 0.9891 - top_3_accuracy: 0.9977 - val_loss: 1.0955 - val_categorical_accuracy: 0.7676 - val_top_2_accuracy: 0.8838 - val_top_3_accuracy: 0.9371\nEpoch 19/30\n907/908 [============================>.] - ETA: 0s - loss: 0.1897 - categorical_accuracy: 0.9481 - top_2_accuracy: 0.9882 - top_3_accuracy: 0.9986\nEpoch 00019: val_top_3_accuracy did not improve from 0.94456\n908/908 [==============================] - 51s 56ms/step - loss: 0.1899 - categorical_accuracy: 0.9479 - top_2_accuracy: 0.9882 - top_3_accuracy: 0.9986 - val_loss: 1.0743 - val_categorical_accuracy: 0.7772 - val_top_2_accuracy: 0.8881 - val_top_3_accuracy: 0.9382\nEpoch 20/30\n907/908 [============================>.] - ETA: 0s - loss: 0.1732 - categorical_accuracy: 0.9525 - top_2_accuracy: 0.9916 - top_3_accuracy: 0.9987\nEpoch 00020: val_top_3_accuracy did not improve from 0.94456\n\nEpoch 00020: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n908/908 [==============================] - 51s 56ms/step - loss: 0.1731 - categorical_accuracy: 0.9525 - top_2_accuracy: 0.9916 - top_3_accuracy: 0.9987 - val_loss: 1.0950 - val_categorical_accuracy: 0.7814 - val_top_2_accuracy: 0.8870 - val_top_3_accuracy: 0.9392\nEpoch 21/30\n907/908 [============================>.] - ETA: 0s - loss: 0.1656 - categorical_accuracy: 0.9516 - top_2_accuracy: 0.9907 - top_3_accuracy: 0.9990\nEpoch 00021: val_top_3_accuracy did not improve from 0.94456\n908/908 [==============================] - 50s 55ms/step - loss: 0.1656 - categorical_accuracy: 0.9516 - top_2_accuracy: 0.9907 - top_3_accuracy: 0.9990 - val_loss: 1.1676 - val_categorical_accuracy: 0.7676 - val_top_2_accuracy: 0.8891 - val_top_3_accuracy: 0.9403\nEpoch 22/30\n906/908 [============================>.] - ETA: 0s - loss: 0.1619 - categorical_accuracy: 0.9561 - top_2_accuracy: 0.9918 - top_3_accuracy: 0.9985\nEpoch 00022: val_top_3_accuracy did not improve from 0.94456\n\nEpoch 00022: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n908/908 [==============================] - 50s 55ms/step - loss: 0.1618 - categorical_accuracy: 0.9561 - top_2_accuracy: 0.9918 - top_3_accuracy: 0.9985 - val_loss: 1.1404 - val_categorical_accuracy: 0.7751 - val_top_2_accuracy: 0.8859 - val_top_3_accuracy: 0.9414\nEpoch 23/30\n907/908 [============================>.] - ETA: 0s - loss: 0.1655 - categorical_accuracy: 0.9537 - top_2_accuracy: 0.9922 - top_3_accuracy: 0.9982\nEpoch 00023: val_top_3_accuracy did not improve from 0.94456\n908/908 [==============================] - 51s 56ms/step - loss: 0.1654 - categorical_accuracy: 0.9537 - top_2_accuracy: 0.9922 - top_3_accuracy: 0.9982 - val_loss: 1.1504 - val_categorical_accuracy: 0.7761 - val_top_2_accuracy: 0.8859 - val_top_3_accuracy: 0.9392\nEpoch 24/30\n907/908 [============================>.] - ETA: 0s - loss: 0.1507 - categorical_accuracy: 0.9580 - top_2_accuracy: 0.9922 - top_3_accuracy: 0.9987\nEpoch 00024: val_top_3_accuracy did not improve from 0.94456\n\nEpoch 00024: ReduceLROnPlateau reducing learning rate to 1e-05.\n908/908 [==============================] - 51s 56ms/step - loss: 0.1512 - categorical_accuracy: 0.9579 - top_2_accuracy: 0.9921 - top_3_accuracy: 0.9987 - val_loss: 1.1504 - val_categorical_accuracy: 0.7729 - val_top_2_accuracy: 0.8859 - val_top_3_accuracy: 0.9403\nEpoch 25/30\n907/908 [============================>.] - ETA: 0s - loss: 0.1586 - categorical_accuracy: 0.9561 - top_2_accuracy: 0.9905 - top_3_accuracy: 0.9980\nEpoch 00025: val_top_3_accuracy did not improve from 0.94456\n908/908 [==============================] - 50s 55ms/step - loss: 0.1584 - categorical_accuracy: 0.9562 - top_2_accuracy: 0.9905 - top_3_accuracy: 0.9980 - val_loss: 1.1373 - val_categorical_accuracy: 0.7740 - val_top_2_accuracy: 0.8902 - val_top_3_accuracy: 0.9414\nEpoch 26/30\n907/908 [============================>.] - ETA: 0s - loss: 0.1540 - categorical_accuracy: 0.9583 - top_2_accuracy: 0.9914 - top_3_accuracy: 0.9982\nEpoch 00026: val_top_3_accuracy did not improve from 0.94456\n908/908 [==============================] - 51s 56ms/step - loss: 0.1540 - categorical_accuracy: 0.9584 - top_2_accuracy: 0.9914 - top_3_accuracy: 0.9982 - val_loss: 1.1551 - val_categorical_accuracy: 0.7708 - val_top_2_accuracy: 0.8870 - val_top_3_accuracy: 0.9414\nEpoch 27/30\n907/908 [============================>.] - ETA: 0s - loss: 0.1520 - categorical_accuracy: 0.9604 - top_2_accuracy: 0.9921 - top_3_accuracy: 0.9985\nEpoch 00027: val_top_3_accuracy did not improve from 0.94456\n908/908 [==============================] - 49s 54ms/step - loss: 0.1522 - categorical_accuracy: 0.9602 - top_2_accuracy: 0.9921 - top_3_accuracy: 0.9985 - val_loss: 1.1415 - val_categorical_accuracy: 0.7697 - val_top_2_accuracy: 0.8881 - val_top_3_accuracy: 0.9392\nEpoch 28/30\n907/908 [============================>.] - ETA: 0s - loss: 0.1506 - categorical_accuracy: 0.9591 - top_2_accuracy: 0.9935 - top_3_accuracy: 0.9991\nEpoch 00028: val_top_3_accuracy did not improve from 0.94456\n908/908 [==============================] - 49s 54ms/step - loss: 0.1505 - categorical_accuracy: 0.9591 - top_2_accuracy: 0.9935 - top_3_accuracy: 0.9991 - val_loss: 1.1647 - val_categorical_accuracy: 0.7697 - val_top_2_accuracy: 0.8859 - val_top_3_accuracy: 0.9403\nEpoch 29/30\n906/908 [============================>.] - ETA: 0s - loss: 0.1580 - categorical_accuracy: 0.9570 - top_2_accuracy: 0.9923 - top_3_accuracy: 0.9987\nEpoch 00029: val_top_3_accuracy did not improve from 0.94456\n908/908 [==============================] - 50s 55ms/step - loss: 0.1577 - categorical_accuracy: 0.9570 - top_2_accuracy: 0.9923 - top_3_accuracy: 0.9987 - val_loss: 1.1614 - val_categorical_accuracy: 0.7719 - val_top_2_accuracy: 0.8891 - val_top_3_accuracy: 0.9424\nEpoch 30/30\n907/908 [============================>.] - ETA: 0s - loss: 0.1604 - categorical_accuracy: 0.9572 - top_2_accuracy: 0.9905 - top_3_accuracy: 0.9988\nEpoch 00030: val_top_3_accuracy did not improve from 0.94456\n908/908 [==============================] - 50s 55ms/step - loss: 0.1604 - categorical_accuracy: 0.9573 - top_2_accuracy: 0.9905 - top_3_accuracy: 0.9988 - val_loss: 1.1649 - val_categorical_accuracy: 0.7697 - val_top_2_accuracy: 0.8849 - val_top_3_accuracy: 0.9382\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the metric names so we can use evaulate_generator\nmodel.metrics_names","execution_count":24,"outputs":[{"output_type":"execute_result","execution_count":24,"data":{"text/plain":"['loss', 'categorical_accuracy', 'top_2_accuracy', 'top_3_accuracy']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here the the last epoch will be used.\n\nval_loss, val_cat_acc, val_top_2_acc, val_top_3_acc = \\\nmodel.evaluate_generator(test_batches, \n                        steps=len(df_val))\n\nprint('val_loss:', val_loss)\nprint('val_cat_acc:', val_cat_acc)\nprint('val_top_2_acc:', val_top_2_acc)\nprint('val_top_3_acc:', val_top_3_acc)","execution_count":25,"outputs":[{"output_type":"stream","text":"val_loss: 0.9794077588509792\nval_cat_acc: 0.7697228\nval_top_2_acc: 0.8848614\nval_top_3_acc: 0.9381663\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here the best epoch will be used.\n\nmodel.load_weights('model.h5')\n\nval_loss, val_cat_acc, val_top_2_acc, val_top_3_acc = \\\nmodel.evaluate_generator(test_batches, \n                        steps=len(df_val))\n\nprint('val_loss:', val_loss)\nprint('val_cat_acc:', val_cat_acc)\nprint('val_top_2_acc:', val_top_2_acc)\nprint('val_top_3_acc:', val_top_3_acc)","execution_count":26,"outputs":[{"output_type":"stream","text":"val_loss: 0.9038912002540807\nval_cat_acc: 0.75479746\nval_top_2_acc: 0.8880597\nval_top_3_acc: 0.9445629\n","name":"stdout"}]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}